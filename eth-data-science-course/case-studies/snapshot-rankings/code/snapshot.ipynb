{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "body {\n",
    "  background-image: url('background_02.png');\n",
    "  background-size: cover;\n",
    "  background-attachment: fixed;\n",
    "  background-repeat: no-repeat;\n",
    "  background-position: center;\n",
    "  margin: 0;\n",
    "  padding: 0;\n",
    "}\n",
    "\n",
    ".main-container {\n",
    "  background-color: rgba(255, 255, 255, 0.975);\n",
    "  padding: 30px;\n",
    "  border-radius: 10px;\n",
    "  max-width: 900px;\n",
    "  margin: auto;\n",
    "  box-shadow: 0px 0px 10px rgba(0,0,0,0.2);\n",
    "}\n",
    "\n",
    "/* Optional: Output box styling */\n",
    ".output-box {\n",
    "  border: 2px solid #18bc9c;\n",
    "  background: #eafaf1;\n",
    "  padding: 12px;\n",
    "  margin: 1em 0;\n",
    "  border-radius: 6px;\n",
    "  font-size: 1.08em;\n",
    "  font-family: \"Fira Mono\", \"Menlo\", \"Monaco\", \"Consolas\", monospace;\n",
    "  white-space: pre-wrap;\n",
    "}\n",
    "</style>\n",
    "<img src=\"../../Shot_Poses_IACS/OmniacPoses_30.png\" width=\"120\" style=\"float:right; margin: 8px 0 8px 20px;\">\n",
    "\n",
    "# Project Overview\n",
    "\n",
    "## Objective\n",
    "\n",
    "This analysis helps **Plurality Labs** and the **Arbitrum DAO** compare strategic framework results on two platforms—**Ethelo** and **Snapshot**. The aim is to determine which channel yields the best quality input. We focus on:\n",
    "\n",
    "- Comparing contributions by delegate and general contributor wallets\n",
    "- Revealing similarities and differences in priorities\n",
    "- Enabling data-driven refinement of the strategic framework\n",
    "- Sharing all results on a public hub\n",
    "\n",
    "## Activities & Timeline\n",
    "\n",
    "- **Data procurement**: Collect aggregate Snapshot data\n",
    "- **Data processing**: Format and clean for analysis\n",
    "- **Feature engineering**: Calculate new summary metrics\n",
    "- **Ranking scheme**: Develop both equal-weighted and token-weighted rankings\n",
    "- **Analysis**: Compare how priorities differ depending on weighting\n",
    "- **Sharing**: Publish data and analysis\n",
    "\n",
    "<img src=\"../../Shot_Poses_IACS/OmniacPoses_31.png\" width=\"120\">\n",
    "\n",
    "# Background\n",
    "\n",
    "During #GovMonth, contributors ranked statements related to “Growth and Innovation” and ways Arbitrum could “Reduce Friction”. Respondents ordered statements by preference. The main questions:\n",
    "\n",
    "**Growth + Innovation Statements**\n",
    "\n",
    "1. Develop accountability practices within ArbitrumDAO.\n",
    "2. Identify the key capabilities for improved DAO performance.\n",
    "3. Form alliances with legacy institutions.\n",
    "4. Fund projects for cross-chain compatibility.\n",
    "5. Improve gas fee optimization.\n",
    "6. Define growth strategies.\n",
    "7. Incentivize users and builders.\n",
    "8. Scale the platform.\n",
    "9. Offer educational opportunities.\n",
    "10. Evolve governance capabilities.\n",
    "\n",
    "**Reducing Friction Statements**\n",
    "\n",
    "1. Build a robust developer community.\n",
    "2. Make Arbitrum more accessible for developers.\n",
    "3. Create an inclusive environment.\n",
    "4. Encourage meaningful DAO participation.\n",
    "5. Raise awareness of opportunities.\n",
    "6. Prioritize gas fee optimization.\n",
    "7. Ensure regulatory compliance.\n",
    "8. Build anti-Sybil protections.\n",
    "9. Uphold transparency.\n",
    "10. Improve token distribution equity.\n",
    "\n",
    "Each respondent's data includes their wallet, rankings, and Arbitrum holdings (for weighting). Below is a snippet of the GovMonth proposal. For this task, voters were prompted with two sets of statements related to “Growth and Innovation” and ways Arbitrum could “Reduce Friction”. The survey respondents were to rank order each statement according to their personal preference. The statements for each are as follows:\n",
    "\n",
    "<img src=\"../../images/govmonth.png\" width=\"600\" style=\"display:block; margin-left:auto; margin-right:auto;\">\n",
    "\n",
    "# Libraries and Setup\n",
    "\n",
    "Below, we load the necessary Python packages for web requests, data wrangling, and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load essential libraries\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import re\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"../../Shot_Poses_IACS/OmniacPoses_33.png\" width=\"150\"/>\n",
    "</div>\n",
    "\n",
    "## Connect to the Snapshot GraphQL API\n",
    "\n",
    "We use `requests` to create a client for querying the Snapshot API. We'll define the queries for:\n",
    "- DAO spaces\n",
    "- Proposals\n",
    "- Votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Snapshot.org GraphQL endpoint\n",
    "SNAPSHOT_URL = \"https://hub.snapshot.org/graphql\"\n",
    "\n",
    "# Prepare GraphQL queries\n",
    "SPACE_DATA_QUERY = \"\"\"\n",
    "query space_data($skip:Int!){ \n",
    "    spaces(orderBy: \"id\", orderDirection: asc,first:1000,skip:$skip){ \n",
    "        id name private about avatar website twitter github coingecko email \n",
    "        network symbol domain proposalsCount activeProposals followersCount \n",
    "        votesCount verified flagged rank \n",
    "    } \n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "PROP_DATA_QUERY = \"\"\"\n",
    "query prop_data($slugid: String!, $timestamp: Int!){ \n",
    "    proposals(orderBy: \"created\", orderDirection: asc,first:1000, \n",
    "        where:{space:$slugid,created_gt:$timestamp}) { \n",
    "        id space{id} ipfs author created network type title body start end \n",
    "        state votes choices scores_state scores \n",
    "    } \n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "VOTE_DATA_QUERY = \"\"\"\n",
    "query vote_data($propid: String!, $timestamp: Int!){ \n",
    "    votes(orderBy: \"created\", orderDirection: asc,first:1000, \n",
    "        where:{proposal:$propid,created_gt:$timestamp}) { \n",
    "        id proposal{id} ipfs voter created choice vp \n",
    "    } \n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "def run_query(query, variables):\n",
    "    \"\"\"Function to execute a GraphQL query.\"\"\"\n",
    "    response = requests.post(SNAPSHOT_URL, json={'query': query, 'variables': variables})\n",
    "    response.raise_for_status()\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right;\">\n",
    "  <img src=\"../../Shot_Poses_IACS/OmniacPoses_34.png\" width=\"150\"/>\n",
    "</div>\n",
    "\n",
    "## Download and Prepare Proposals\n",
    "\n",
    "We'll now write functions to download:\n",
    "- All proposals in a space (filtered to two proposals of interest)\n",
    "- All votes for a proposal\n",
    "\n",
    "Each function contains step-by-step comments explaining the loop logic and purpose. We then filter the proposals to the two relevant ones for the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_proposals(slug):\n",
    "    \"\"\"Function to fetch all proposals for a given DAO space.\"\"\"\n",
    "    c_timestamp = 0\n",
    "    prop_data = []\n",
    "    while True:\n",
    "        variables = {'slugid': slug, 'timestamp': c_timestamp}\n",
    "        result = run_query(PROP_DATA_QUERY, variables)\n",
    "        proposals = result.get('data', {}).get('proposals', [])\n",
    "        if not proposals:\n",
    "            break\n",
    "        prop_data.extend(proposals)\n",
    "        c_timestamp = int(proposals[-1]['created'])\n",
    "        # print(f\"Fetched {len(prop_data)} Entries\")\n",
    "    \n",
    "    df = pd.json_normalize(prop_data)\n",
    "    df.rename(columns={'space.id': 'space_id'}, inplace=True)\n",
    "    return df.drop(columns=['space'])\n",
    "\n",
    "# Get proposals for Arbitrum DAO\n",
    "prop_df = get_proposals(\"arbitrumfoundation.eth\")\n",
    "\n",
    "# Focus on two proposals of interest\n",
    "proposal_ids = [\n",
    "    \"0x14e71f784e880170972572c2696ef53ef437700c637a151b5176a5827fe5b8bc\",\n",
    "    \"0x5824d0b51cc435a49f6455ee2715216d6b958637218ed79e3e93c41af6bdef33\"\n",
    "]\n",
    "prop_df_sub = prop_df[prop_df['id'].isin(proposal_ids)].copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and Prepare Vote Data\n",
    "\n",
    "This function retrieves all votes for each proposal. Pagination is used to ensure we get all data, and results are stored in a list for each proposal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_votes(prop_id):\n",
    "    \"\"\"Function to fetch all votes for a given proposal ID.\"\"\"\n",
    "    c_timestamp = 0\n",
    "    vote_data = []\n",
    "    while True:\n",
    "        variables = {'propid': prop_id, 'timestamp': c_timestamp}\n",
    "        result = run_query(VOTE_DATA_QUERY, variables)\n",
    "        votes = result.get('data', {}).get('votes', [])\n",
    "        if not votes:\n",
    "            break\n",
    "        vote_data.extend(votes)\n",
    "        c_timestamp = int(votes[-1]['created'])\n",
    "        # print(f\"Fetched {len(vote_data)} Entries for proposal {prop_id}\") # uncomment to track progress\n",
    "    df = pd.json_normalize(vote_data)\n",
    "    df.rename(columns={'proposal.id': 'prop_id'}, inplace=True)\n",
    "    return df.drop(columns=['proposal'])\n",
    "\n",
    "# Download votes for both proposals and combine into one dataframe\n",
    "vote_l = [get_votes(pid) for pid in prop_df_sub['id']]\n",
    "vote_df = pd.concat(vote_l, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../../Shot_Poses_IACS/OmniacPoses_35.png\" width=\"120\" style=\"float:right; margin: 8px 0 8px 20px;\">\n",
    "\n",
    "# Analyze and Rank Statements\n",
    "\n",
    "The next step is to calculate various summary statistics for each statement in each proposal, both by unweighted and token-weighted votes. Comments clarify every operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vote_df14 = vote_df[vote_df['prop_id'] == \"0x14e71f784e880170972572c2696ef53ef437700c637a151b5176a5827fe5b8bc\"].copy()\n",
    "vote_df58 = vote_df[vote_df['prop_id'] == \"0x5824d0b51cc435a49f6455ee2715216d6b958637218ed79e3e93c41af6bdef33\"].copy()\n",
    "\n",
    "def compute_ranking(vote_df):\n",
    "    \"\"\"Helper to unpack choices and compute stats.\"\"\"\n",
    "    n_choices = 10\n",
    "    # The 'choice' column is a list of lists, we need to convert it to a NumPy array of ranks\n",
    "    rankings = np.array([np.argsort(c) + 1 for c in vote_df['choice']])\n",
    "    \n",
    "    # Ensure vp is numeric and handle potential NaNs\n",
    "    vote_df['vp'] = pd.to_numeric(vote_df['vp'], errors='coerce').fillna(0)\n",
    "    total_vp = vote_df['vp'].sum()\n",
    "    vp_weights = vote_df['vp'] / total_vp if total_vp > 0 else np.zeros(len(vote_df))\n",
    "\n",
    "    results = []\n",
    "    for i in range(1, n_choices + 1):\n",
    "        # Find where choice 'i' is in each ranking\n",
    "        ranks_for_choice_i = np.array([np.where(row == i)[0][0] + 1 for row in np.array(list(vote_df['choice']))])\n",
    "        \n",
    "        pct_first = np.mean([c[0] == i for c in vote_df['choice']])\n",
    "        pct_last = np.mean([c[-1] == i for c in vote_df['choice']])\n",
    "        pct_first_five = np.mean([i in c[:5] for c in vote_df['choice']])\n",
    "        \n",
    "        results.append({\n",
    "            'Choice': i,\n",
    "            'SumRank': np.sum(ranks_for_choice_i),\n",
    "            'SumRankAvg': np.mean(ranks_for_choice_i),\n",
    "            'PctFirst': pct_first,\n",
    "            'PctLast': pct_last,\n",
    "            'PctFirstFive': pct_first_five,\n",
    "            'VPRankAvg': np.sum(ranks_for_choice_i * vp_weights)\n",
    "        })\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "rankraw14 = compute_ranking(vote_df14)\n",
    "rankraw58 = compute_ranking(vote_df58)\n",
    "\n",
    "print(\"Growth & Innovation Rankings\")\n",
    "display(rankraw14.sort_values(by='SumRank'))\n",
    "print(\"\\nReducing Friction Rankings\")\n",
    "display(rankraw58.sort_values(by='SumRank'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../../Shot_Poses_IACS/OmniacPoses_38.png\" width=\"120\" style=\"float:left; margin: 8px 0 8px 20px;\">\n",
    "\n",
    "## Discussion and Insights\n",
    "**Example Interpretation**\n",
    "For \"Reducing Friction\", \"Prioritize Gas Fee Optimization\" was ranked highest when votes are equally weighted.\n",
    "\n",
    "When votes are weighted by tokens, \"Build a robust community of developers\" rises to the top.\n",
    "\n",
    "This shows that community and large holders align on the top priorities, but token weighting can shift the relative importance of specific statements.\n",
    "\n",
    "## Visualizing Rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ranking(rankdf, title):\n",
    "    \"\"\"Function to plot the rankings.\"\"\"\n",
    "    df = rankdf.copy()\n",
    "    df['Statement'] = [f\"Choice {i}\" for i in df['Choice']]\n",
    "    df_melted = df.melt(id_vars='Statement', value_vars=['SumRankAvg', 'VPRankAvg'], \n",
    "                          var_name='Type', value_name='Value')\n",
    "    \n",
    "    plt.figure(figsize=(12, 7))\n",
    "    sns.barplot(data=df_melted, x='Statement', y='Value', hue='Type', palette='viridis')\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.ylabel('Average Rank')\n",
    "    plt.xlabel('')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_ranking(rankraw14, \"Growth & Innovation: Equal vs Token Weighted Rankings\")\n",
    "plot_ranking(rankraw58, \"Reducing Friction: Equal vs Token Weighted Rankings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"../../Shot_Poses_IACS/OmniacPoses_36.png\" width=\"150\"/>\n",
    "</div>\n",
    "\n",
    "# OpenAI Analysis\n",
    "\n",
    "In this section, we integrate advanced language modeling into our DAO voting analysis. While previous sections relied on deterministic statistical aggregation, here we leverage OpenAI’s GPT-4o-mini model to synthesize a group consensus ranking from raw voting data. This approach introduces a qualitative perspective and allows us to compare traditional quantitative methods with the “collective intelligence” that an LLM might infer from observed voter preferences.\n",
    "\n",
    "<div style=\"text-align: right;\">\n",
    "  <img src=\"../../Shot_Poses_IACS/OmniacPoses_37.png\" width=\"150\"/>\n",
    "</div>\n",
    "\n",
    "## Extracting Choice Labels Programmatically\n",
    "\n",
    "Before we can prompt OpenAI, we need to translate the numeric rankings from each voter back to their original statement text. The statement text for each ranked-choice poll is stored in the proposal body field, but not in a direct table. The following function automates extraction by parsing the proposal text for enumerated statements. This ensures our analysis is robust and future-proof, as it adapts to any poll with statements following a consistent numbering format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_choice_labels(proposal_body):\n",
    "    \"\"\"Extracts the choice labels (statements) from a Snapshot proposal's body text.\"\"\"\n",
    "    try:\n",
    "        # Find the block starting with a variation of the statements header\n",
    "        start_match = re.search(r\"These are the statements:\", proposal_body, re.IGNORECASE)\n",
    "        if not start_match:\n",
    "            raise ValueError(\"Can't find statement block in proposal body\")\n",
    "        \n",
    "        statements_block = proposal_body[start_match.start():]\n",
    "        \n",
    "        # Find all lines starting with '1.', '2.', etc.\n",
    "        lines = statements_block.split('\\n')\n",
    "        choice_lines = [line for line in lines if re.match(r\"^\\d+\\.\\s+\", line.strip())]\n",
    "        \n",
    "        # Remove numbering to get the final labels\n",
    "        labels = [re.sub(r\"^\\d+\\.\\s*\", \"\", line).strip() for line in choice_lines]\n",
    "        return labels\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting labels: {e}\")\n",
    "        return []\n",
    "\n",
    "# Example: for vote_df14 (growth & innovation)\n",
    "proposal_id_14 = vote_df14['prop_id'].unique()[0]\n",
    "proposal_body_14 = prop_df_sub.loc[prop_df_sub['id'] == proposal_id_14, 'body'].iloc[0]\n",
    "choice_labels_14 = extract_choice_labels(proposal_body_14)\n",
    "\n",
    "# For vote_df58 (reducing friction)\n",
    "proposal_id_58 = vote_df58['prop_id'].unique()[0]\n",
    "proposal_body_58 = prop_df_sub.loc[prop_df_sub['id'] == proposal_id_58, 'body'].iloc[0]\n",
    "choice_labels_58 = extract_choice_labels(proposal_body_58)\n",
    "\n",
    "print(\"--- Growth & Innovation Labels ---\")\n",
    "print(choice_labels_14)\n",
    "print(\"\\n--- Reducing Friction Labels ---\")\n",
    "print(choice_labels_58)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Consensus Ranking Prompt for OpenAI\n",
    "To minimize costs and maximize efficiency when using the OpenAI API, we sample a subset of the available voter rankings. This random sampling reduces the number of tokens sent, while still capturing the main trends in voter sentiment. The following function packages this sample, constructs a precise API prompt, and defines exactly how we expect GPT-4o-mini to return results. Importantly, the prompt asks the model not just for a ranking, but also for a brief explanation of its methodology—providing valuable transparency into its “reasoning” process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_consensus_ranking(vote_df, prop_df_sub, n_sample=30, api_key_path=\"your_openai_key.txt\"):\n",
    "    \"\"\"Main function: Get consensus ranking using OpenAI.\"\"\"\n",
    "    # a. Find proposal body for this vote_df\n",
    "    proposal_id = vote_df['prop_id'].unique()[0]\n",
    "    proposal_body = prop_df_sub.loc[prop_df_sub['id'] == proposal_id, 'body'].iloc[0]\n",
    "    if not proposal_body:\n",
    "        raise ValueError(\"Proposal body not found or ambiguous!\")\n",
    "    choice_labels = extract_choice_labels(proposal_body)\n",
    "\n",
    "    # b. Prepare numeric rankings (sample for token efficiency)\n",
    "    numeric_rankings_text = [','.join(map(str, x)) for x in vote_df['choice']]\n",
    "    np.random.seed(42)\n",
    "    sample_indices = np.random.choice(len(numeric_rankings_text), size=min(n_sample, len(numeric_rankings_text)), replace=False)\n",
    "    sampled_rankings = [numeric_rankings_text[i] for i in sample_indices]\n",
    "    all_numeric_text = \"\\n\".join(sampled_rankings)\n",
    "\n",
    "    # Build minimal prompt for OpenAI\n",
    "    prompt = (\n",
    "        \"You are an expert at group consensus analysis. \"\n",
    "        \"Below are multiple ranked lists of choices, each a permutation of the numbers 1–10, where each line is a different voter. \"\n",
    "        \"Each number corresponds to a unique statement (but you do not need to know the statements).\\n\\n\"\n",
    "        \"Each line is the ranked order for one voter (first is most preferred, last is least):\\n\"\n",
    "        f\"{all_numeric_text}\\n\\n\"\n",
    "        \"Your tasks:\\n\"\n",
    "        \"1. Based only on these rankings, synthesize a single consensus ranking (as a permutation of 1–10) that best reflects the collective preference.\\n\"\n",
    "        \"2. Output ONLY the consensus ranking as your first line, in the following format (with no explanation before):\\n\"\n",
    "        \"Consensus: x1,x2,x3,x4,x5,x6,x7,x8,x9,x10\\n\"\n",
    "        \"3. After the ranking, in 2-4 sentences, explain *how* you synthesized this ranking: mention if you averaged positions, looked for patterns, considered polarization, etc. Focus on what you observed in the rankings, not the meaning of the numbers.\"\n",
    "    )\n",
    "\n",
    "    # Send to OpenAI API\n",
    "    try:\n",
    "        with open(api_key_path, 'r') as f:\n",
    "            openai_api_key = f.read().strip()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"API key file not found at: {api_key_path}\")\n",
    "        return None\n",
    "\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {openai_api_key}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    data = {\n",
    "        \"model\": \"gpt-4o-mini\",\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "        \"temperature\": 0.2,\n",
    "        \"max_tokens\": 150\n",
    "    }\n",
    "    response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=data)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error from OpenAI API: {response.status_code} - {response.text}\")\n",
    "        return None\n",
    "\n",
    "    # Parse response, map consensus to text\n",
    "    result = response.json()\n",
    "    content = result['choices'][0]['message']['content']\n",
    "    lines = content.strip().split('\\n')\n",
    "    num_line_match = re.search(r\"^Consensus:\\s*([\\d,]+)\", lines[0])\n",
    "\n",
    "    if not num_line_match:\n",
    "        print(\"No consensus ranking found in the model output.\")\n",
    "        print(content)\n",
    "        return None\n",
    "    \n",
    "    consensus_numeric_str = num_line_match.group(1)\n",
    "    consensus_ids = [int(x) for x in consensus_numeric_str.split(',')]\n",
    "    consensus_statements = [choice_labels[i-1] for i in consensus_ids]\n",
    "\n",
    "    print(\"Consensus Ranking (Numbers):\\n\", consensus_numeric_str, \"\\n\")\n",
    "    print(\"Consensus Ranking (Statements):\")\n",
    "    for i, (cid, statement) in enumerate(zip(consensus_ids, consensus_statements)):\n",
    "        print(f\"Rank {i+1}: Choice {cid}: {statement}\")\n",
    "    \n",
    "    print(\"\\n---\\nOpenAI Model Explanation:\")\n",
    "    explanation = \"\\n\".join(lines[1:]).strip()\n",
    "    print(explanation, \"\\n\")\n",
    "    return consensus_statements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left;\">\n",
    "  <img src=\"../../Shot_Poses_IACS/OmniacPoses_39.png\" width=\"150\"/>\n",
    "</div>\n",
    "\n",
    "## Calling the GPT-4o-mini Consensus Function\n",
    "\n",
    "Now, we call our custom function, passing in the appropriate vote and proposal data. The output will include both the ranked statements (mapped from the original choice indices) and the model's brief explanation of its ranking strategy.\n",
    "\n",
    "By comparing these results to our deterministic, token-weighted rankings, we can highlight the differences between a purely statistical aggregation and the type of pattern recognition and consensus modeling performed by a state-of-the-art language model. This comparison adds another layer of interpretability to our study and offers a compelling way to triangulate DAO preferences.\n",
    "\n",
    "*Note: API costs can scale with prompt size, so be mindful of your sample size and your OpenAI account limits.*\n",
    "\n",
    "### Interpretation\n",
    "The GPT-4o-mini model's ranking is sometimes different from our classic deterministic aggregation. This is partly due to random sampling (to save tokens/cost) but also reflects the LLM's ability to weigh not only strict averages but also patterns in the distribution of ranks—such as consistency, outliers, and relative ordering trends. Its brief \"reasoning\" section can highlight patterns we might otherwise miss, making it a useful supplement to classic data science methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- For proposal 14 (Growth & Innovation) --\n",
    "print(\"--- Analyzing Growth & Innovation ---\")\n",
    "consensus_14 = get_consensus_ranking(vote_df14, prop_df_sub, n_sample=500, api_key_path=\"../../openai_key.txt\")\n",
    "\n",
    "# -- For proposal 58 (Reducing Friction) --\n",
    "print(\"\\n--- Analyzing Reducing Friction ---\")\n",
    "consensus_58 = get_consensus_ranking(vote_df58, prop_df_sub, n_sample=500, api_key_path=\"../../openai_key.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right;\">\n",
    "  <img src=\"../../Shot_Poses_IACS/OmniacPoses_41.png\" width=\"150\"/>\n",
    "</div>\n",
    "\n",
    "# Conclusions\n",
    "\n",
    "This analysis provides a comprehensive framework for quantifying and understanding alignment between large token holders and the broader Arbitrum DAO community with respect to strategic priorities. By examining both equal-weighted (one wallet, one vote) and token-weighted (voting power) rankings, we are able to reveal not only the overall consensus but also to highlight potential divergences between high-stake holders and general participants.\n",
    "\n",
    "Through classic statistical aggregation, we demonstrated which statements consistently ranked at the top across different voting schemes, thereby identifying areas of strong community consensus. Conversely, discrepancies between the two schemes can help surface priorities that are particularly important to either major stakeholders or the broader base, informing more inclusive decision-making processes.\n",
    "\n",
    "In addition to these quantitative approaches, we integrated OpenAI's state-of-the-art language model (GPT-4o-mini) into our workflow. By prompting the AI with real anonymized voting records, we obtained an AI-synthesized consensus ranking and a concise explanation of the logic behind the ordering. This adds a qualitative layer of analysis that can capture subtle patterns—such as clustering, polarization, or emergent trends in ranking behavior—that might be overlooked by traditional methods. The AI explanation also enhances transparency and interpretability, offering an alternative lens for both researchers and governance participants.\n",
    "\n",
    "Our methodology is generalizable and can be readily applied to other DAO governance polls and ranking-based decision scenarios. By blending statistical analysis with AI-based synthesis, we offer a richer, multidimensional perspective on community preferences—empowering DAOs to make more informed, balanced, and democratic decisions.\n",
    "\n",
    "*In summary, combining traditional ranking analytics with modern AI tools not only validates results through multiple lenses but also brings greater transparency and insight into complex governance ecosystems like Arbitrum DAO.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}