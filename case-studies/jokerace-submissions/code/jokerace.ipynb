{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "/* Make sure body and html fill screen */\n",
    "html, body {\n",
    "  height: 100%;\n",
    "  margin: 0;\n",
    "  padding: 0;\n",
    "}\n",
    "\n",
    "/* Background image */\n",
    "body::before {\n",
    "  content: \"\";\n",
    "  position: fixed;\n",
    "  top: 0;\n",
    "  left: 0;\n",
    "  z-index: -1;\n",
    "  width: 100%;\n",
    "  height: 100%;\n",
    "  background-image: url('background_01.png');\n",
    "  background-size: cover;\n",
    "  background-repeat: no-repeat;\n",
    "  background-attachment: fixed;\n",
    "  background-position: center;\n",
    "  opacity: 1;\n",
    "}\n",
    "\n",
    "/* Style the main notebook content container */\n",
    "#notebook-container, .container {\n",
    "  background-color: rgba(255, 255, 255, 0.97); /* or #ffffff for solid */\n",
    "  padding: 40px;\n",
    "  border-radius: 10px;\n",
    "  max-width: 1000px;\n",
    "  margin: 40px auto;\n",
    "  box-shadow: 0px 0px 12px rgba(0,0,0,0.15);\n",
    "}\n",
    "\n",
    "/* Optional: Responsive tweaks */\n",
    "@media (max-width: 768px) {\n",
    "  #notebook-container, .container {\n",
    "    padding: 20px;\n",
    "  }\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Jokerace Submissions: Vision, Mission, and Strategic Priorities\n",
    "**Author**: Omniacs.DAO\n",
    "<br>\n",
    "<img src=\"Shot_Poses_IACS/OmniacPoses_23.png\" width=\"120\" style=\"float:right; margin: 8px 0 8px 20px;\">\n",
    "\n",
    "### Case Study: From Raw Text to Actionable Insights with Python and OpenAI\n",
    "How can you make sense of thousands of free-text submissions from the Arbitrum community? This is a common challenge for decentralized autonomous organizations (DAOs), companies, and projects that rely on community feedback to guide their strategy. Raw text is messy, unstructured, and time-consuming to analyze manually.\n",
    "\n",
    "In this case study, we'll walk through a complete, real-world workflow for analyzing community proposal submissions from the Arbitrum DAO's Jokerace contests. We will use a powerful combination of classic NLP techniques and modern AI to extract meaningful themes and generate a high-level summary.\n",
    "\n",
    "You will learn how to:\n",
    "\n",
    "1. **Preprocess and Clean** raw text data for analysis.\n",
    "2. Use **N-gram Analysis** to find common multi-word phrases.\n",
    "3. Apply **Topic Modeling (LDA)** to discover latent themes in the submissions.\n",
    "4. Leverage the **OpenAI API** to synthesize your findings into a concise, human-readable report.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"Shot_Poses_IACS/OmniacPoses_02.png\" width=\"150\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Project Overview and Setup\n",
    "**Objective**\n",
    "\n",
    "Plurality Labs, founded by @DisruptionJoe and supported by his teammates @prose11 | GovAlpha and @radioflyerMQ were given the task to setup experimental governance mechanisms that would allow for formalized \"sense-making\" that will help the DAO come up with a strong mission and vision to guide their decisions as they then follow up with initiating the capital allocation process where grants will be given out to support the ARB ecosystem.\n",
    "\n",
    "One of those experiments was an online incentivized survey hosted by \"JokerRace\" as part of the ThanksARB initiative of Arbitrum's \"#GOVMonth\". The Jokeraces consisted of 4 separate on-chain surveys where users were prompted to provide feedback on:\n",
    "\n",
    "- ArbitrumDAO Short-term Strategic Priorities (Reduce Friction) - `0xbf47BDA4b172daf321148197700cBED04dbe0D58`\n",
    "- ArbitrumDAO Long-term Strategic Priorities (Growth and Innovation) - `0x5D4e25fA847430Bf1974637f5bA8CB09D0B94ec7`\n",
    "- ArbitrumDAO Northstar Strategic Framework: Vision - `0x0d4C05e4BaE5eE625aADC35479Cc0b140DDF95D4`\n",
    "- ArbitrumDAO Northstar Strategic Framework: Mission - `0x5a207fA8e1136303Fd5232E200Ca30042c45c3B6`\n",
    "\n",
    "The goal of this case-study is to analyze submissions across four Jokerace contests—Vision, Mission, Short-term priorities, and Long-term priorities—to:\n",
    "\n",
    "1. Identify key themes, concerns, and strategic ideas proposed by the community.\n",
    "2. Develop a reusable methodology for future text analysis.\n",
    "3. Create a summarized report of the findings.\n",
    "\n",
    "**Python Environment Setup**\n",
    "\n",
    "First, let's set up our Python environment by loading the necessary libraries. These packages cover everything from data manipulation (`pandas`) and text mining (`nltk`, `scikit-learn`) to API communication (`requests`).\n",
    "\n",
    "To install the required packages, run:\n",
    "```bash\n",
    "pip install pandas scikit-learn nltk beautifulsoup4 requests\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load Libraries ---\n",
    "# For data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "\n",
    "# For text mining and NLP\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# For interacting with APIs\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# For display purposes in Jupyter\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# --- Initial Setup ---\n",
    "# Download necessary NLTK data (if not already downloaded)\n",
    "try:\n",
    "    stopwords.words('english')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "# Initialize stemmer\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading the Data**\n",
    "\n",
    "We'll use one main dataset:\n",
    "`propfinal`: Contains the core proposal submissions, including the text content, author, and timestamp from JokerAce.\n",
    "\n",
    "*(Note: The original `.RDS` file has been converted to a more Python-friendly `.csv` format for this notebook.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "# The original R script used readRDS. We'll use pandas to read a CSV or Pickle file.\n",
    "# Ensure the data file 'DataProposalsJokerAce.csv' is in a '../data/' directory relative to this notebook.\n",
    "try:\n",
    "    propfinal = pd.read_csv('../data/DataProposalsJokerAce.csv')\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Data file not found. Please ensure 'DataProposalsJokerAce.csv' is in the '../data/' directory.\")\n",
    "    # Create a dummy dataframe for demonstration purposes if file doesn't exist\n",
    "    propfinal = pd.DataFrame({\n",
    "        'Content': ['<p>Example submission 1 about fixing bugs.</p>', 'Another entry about community growth.', 'Improve the user experience by reducing fees.'],\n",
    "        'Contract': ['0xbf47bda4b172daf321148197700cbed04dbe0d58', '0x5d4e25fa847430bf1974637f5ba8cb09d0b94ec7', '0xbf47bda4b172daf321148197700cbed04dbe0d58'],\n",
    "        'ContentParsed': ['Example submission 1 about fixing bugs.', 'Another entry about community growth.', 'Improve the user experience by reducing fees.']\n",
    "    })\n",
    "\n",
    "display(propfinal.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left;\">\n",
    "  <img src=\"Shot_Poses_IACS/OmniacPoses_03.png\" width=\"150\"/>\n",
    "</div>\n",
    "\n",
    "### 2. Data Cleaning and Preprocessing\n",
    "\n",
    "Text data from the wild is messy. It's filled with HTML tags, punctuation, and inconsistent capitalization. To prepare it for analysis, we need to standardize it through a process called preprocessing. Our pipeline will:\n",
    "\n",
    "1. Remove HTML tags.\n",
    "2. Convert all text to lowercase.\n",
    "3. Remove punctuation.\n",
    "4. Stem words to their root form (e.g., \"running\", \"ran\", and \"runs\" all become \"run\"). This helps group related words.\n",
    "5. Trim excess whitespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Clean and Preprocess Text ---\n",
    "\n",
    "def clean_text(text):\n",
    "    # 1. Remove HTML tags\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "    # 2. Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # 3. Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # 4. Stem words\n",
    "    text = ' '.join([stemmer.stem(word) for word in text.split()])\n",
    "    # 5. Trim whitespace\n",
    "    return text.strip()\n",
    "\n",
    "# Apply the cleaning function to the 'Content' column\n",
    "propfinal_clean = propfinal['Content'].apply(clean_text)\n",
    "\n",
    "# Display a before-and-after comparison\n",
    "df_preview = pd.DataFrame({\n",
    "    'Raw': propfinal['Content'].head(),\n",
    "    'Cleaned': propfinal_clean.head()\n",
    "})\n",
    "\n",
    "display(df_preview)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This clean, stemmed text is now ready for more advanced analysis.\n",
    "\n",
    "<div style=\"text-align: left;\">\n",
    "  <img src=\"Shot_Poses_IACS/OmniacPoses_04.png\" width=\"150\"/>\n",
    "</div>\n",
    "\n",
    "## 3. Exploratory Analysis: N-grams\n",
    "What are the most common phrases in the submissions? While looking at single words (unigrams) is useful, n-grams (sequences of n words) give us more context. For example, the trigram \"reduce transaction fees\" is far more insightful than the individual words \"reduce,\" \"transaction,\" and \"fees.\"\n",
    "\n",
    "Let's find the most common trigrams (3-word phrases), excluding common \"stop words\" like \"the,\" \"a,\" and \"is\". We'll also add custom stop words specific to our dataset, like \"arbitrum\" and \"dao,\" to filter out noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- N-gram Analysis ---\n",
    "\n",
    "# Define stopwords\n",
    "custom_stopwords = ['arbitrumdao', 'arbitrum', 'project', 'arb', 'dao']\n",
    "stop_words = list(stopwords.words('english')) + custom_stopwords\n",
    "\n",
    "# Use CountVectorizer to get trigram counts\n",
    "vec = CountVectorizer(ngram_range=(3, 3), stop_words=stop_words).fit(propfinal_clean)\n",
    "bag_of_words = vec.transform(propfinal_clean)\n",
    "sum_words = bag_of_words.sum(axis=0)\n",
    "words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "trigram_table = sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "\n",
    "# Format as a DataFrame for display\n",
    "trigram_df = pd.DataFrame(trigram_table, columns=['Trigram', 'Count'])\n",
    "\n",
    "print(\"Top 15 Trigrams:\")\n",
    "display(trigram_df.head(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This output immediately gives us a feel for recurring suggestions and ideas within the community.\n",
    "\n",
    "<img src=\"Shot_Poses_IACS/OmniacPoses_05.png\" width=\"120\" style=\"float:left; margin: 8px 20px 8px 0;\">\n",
    "\n",
    "## 4. Uncovering Themes with Topic Modeling (LDA)\n",
    "\n",
    "While n-grams show us popular phrases, topic modeling helps us discover the underlying, latent themes across all submissions. We'll use Latent Dirichlet Allocation (LDA), a popular unsupervised algorithm that works by:\n",
    "\n",
    "1. Assuming each document is a mix of topics.\n",
    "2. Assuming each topic is a mix of words.\n",
    "3. Figuring out the \"topics\" (which are just clusters of words) that best explain the collection of documents.\n",
    "\n",
    "**The LDA Workflow**\n",
    "\n",
    "We'll create a repeatable workflow to process each contest's submissions.\n",
    "\n",
    "**Step 1**: Define Helper Functions\n",
    "\n",
    "We'll create functions to vectorize the text into a Document-Term Matrix (DTM) and run the LDA model. A DTM is a simple matrix where rows represent documents, columns represent terms (words), and the cells contain the word counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- LDA Helper Functions ---\n",
    "\n",
    "def create_dtm(texts, stop_words_list, min_freq=5):\n",
    "    \"\"\"Creates a Document-Term Matrix from a list of texts.\"\"\"\n",
    "    vectorizer = CountVectorizer(\n",
    "        stop_words=stop_words_list,\n",
    "        min_df=min_freq, # equivalent to global bounds in tm\n",
    "        preprocessor=lambda x: re.sub(r'\\d+', '', x.lower()), # remove numbers and lowercase\n",
    "        tokenizer=lambda x: [stemmer.stem(w) for w in re.split(r'\\W+', x) if w] # stem and tokenize\n",
    "    )\n",
    "    dtm = vectorizer.fit_transform(texts)\n",
    "    # Filter out documents that become empty after preprocessing\n",
    "    non_empty_docs_mask = np.asarray(dtm.sum(axis=1)).flatten() > 0\n",
    "    return dtm[non_empty_docs_mask, :], vectorizer, non_empty_docs_mask\n",
    "\n",
    "def run_lda(dtm, K, iterations=50):\n",
    "    \"\"\"Runs the LDA algorithm.\"\"\"\n",
    "    lda_model = LatentDirichletAllocation(\n",
    "        n_components=K, \n",
    "        random_state=1234, \n",
    "        max_iter=iterations,\n",
    "        learning_method='online' # similar to 'Gibbs'\n",
    "    )\n",
    "    lda_model.fit(dtm)\n",
    "    return lda_model\n",
    "\n",
    "def get_top_terms(model, feature_names, n=20):\n",
    "    \"\"\"Extracts top terms from the LDA model.\"\"\"\n",
    "    top_terms = {}\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_terms[f\"Topic_{topic_idx + 1}\"] = [feature_names[i] for i in topic.argsort()[:-n - 1:-1]]\n",
    "    return pd.DataFrame(top_terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2**: Set Up and Run the Analysis Loop\n",
    "\n",
    "Now, we define the contests we want to analyze and the number of topics (K) we want to find for each. Choosing K is a mix of art and science; it often requires experimentation to find a number that produces coherent, distinct topics.\n",
    "\n",
    "The loop will perform the following for each contest:\n",
    "\n",
    "1. Subset the proposals for that contest.\n",
    "2. Perform final cleaning and filtering.\n",
    "3. Create the DTM using our helper function.\n",
    "4. Run the LDA model.\n",
    "5. Assign the most likely topic to each proposal.\n",
    "6. Extract and save the top words for each discovered topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define Contests and K (Number of Topics) ---\n",
    "contest_data = [\n",
    "  {'Address': \"0xbf47bda4b172daf321148197700cbed04dbe0d58\", 'Name': \"Reduce Friction\", 'Slug': \"RF4\", 'Topics': 4},\n",
    "  {'Address': \"0x5d4e25fa847430bf1974637f5ba8cb09d0b94ec7\", 'Name': \"Growth and Innovation\", 'Slug': \"GI5\", 'Topics': 5},\n",
    "  {'Address': \"0x0d4c05e4bae5ee625aadc35479cc0b140ddf95d4\", 'Name': \"Vision\", 'Slug': \"V7\", 'Topics': 7},\n",
    "  {'Address': \"0x0d4c05e4bae5ee625aadc35479cc0b140ddf95d4\", 'Name': \"Vision\", 'Slug': \"V10\", 'Topics': 10},\n",
    "  {'Address': \"0x5a207fa8e1136303fd5232e200ca30042c45c3b6\", 'Name': \"Mission\", 'Slug': \"M11\", 'Topics': 11},\n",
    "  {'Address': \"0x5a207fa8e1136303fd5232e200ca30042c45c3b6\", 'Name': \"Mission\", 'Slug': \"M15\", 'Topics': 15}\n",
    "]\n",
    "contestdf = pd.DataFrame(contest_data)\n",
    "\n",
    "# Create a directory to store outputs\n",
    "output_dir = \"../project1/temp/txts\"\n",
    "for slug in contestdf['Slug']:\n",
    "    os.makedirs(os.path.join(output_dir, slug), exist_ok=True)\n",
    "\n",
    "# --- Main LDA Analysis Loop ---\n",
    "all_results = []\n",
    "propout = propfinal.copy()\n",
    "\n",
    "for idx, contest in contestdf.iterrows():\n",
    "    print(f\"Processing: {contest['Name']} ({contest['Slug']})\")\n",
    "    # 1. Subset data for this contest\n",
    "    subset_df = propfinal[propfinal['Contract'] == contest['Address']].copy()\n",
    "    \n",
    "    # 2. Clean and filter text\n",
    "    # Fix encoding, filter by length, and remove duplicates\n",
    "    subset_df['ContentParsed'] = subset_df['ContentParsed'].str.encode('ascii', 'ignore').str.decode('ascii')\n",
    "    subset_df = subset_df[subset_df['ContentParsed'].str.len() > 40]\n",
    "    subset_df = subset_df.drop_duplicates(subset=['ContentParsed'])\n",
    "    \n",
    "    textdata = subset_df['ContentParsed']\n",
    "\n",
    "    if len(textdata) < contest['Topics']:\n",
    "        print(f\"  Skipping {contest['Slug']}: Not enough documents.\")\n",
    "        continue\n",
    "\n",
    "    # 3. Create DTM\n",
    "    dtm, vectorizer, valid_mask = create_dtm(textdata, stop_words, min_freq=5)\n",
    "    docdf = subset_df.loc[valid_mask].copy()\n",
    "\n",
    "    if dtm.shape[0] < contest['Topics']:\n",
    "        print(f\"  Skipping {contest['Slug']}: Not enough documents after pruning.\")\n",
    "        continue\n",
    "\n",
    "    # 4. Topic modeling\n",
    "    K = contest['Topics']\n",
    "    lda_model = run_lda(dtm, K)\n",
    "    topic_results = lda_model.transform(dtm)\n",
    "    docdf['Topic'] = topic_results.argmax(axis=1) + 1 # 1-based topic index\n",
    "    \n",
    "    # 5. Annotate main dataframe with topic assignment\n",
    "    propout.loc[docdf.index, contest['Slug']] = docdf['Topic']\n",
    "\n",
    "    # 6. Save per-topic submissions as text files\n",
    "    for topic_num in docdf['Topic'].unique():\n",
    "        tempd = docdf[docdf['Topic'] == topic_num]\n",
    "        out_path = os.path.join(output_dir, contest['Slug'], f\"Topic-{topic_num}.txt\")\n",
    "        with open(out_path, 'w', encoding='utf-8') as f:\n",
    "            for i, row in tempd.iterrows():\n",
    "                f.write(f\"Submission : {i}\\n{row['ContentParsed']}\\n\\n\\n\")\n",
    "\n",
    "    # 7. Save top terms\n",
    "    top_terms_df = get_top_terms(lda_model, vectorizer.get_feature_names(), n=50)\n",
    "    top_terms_df.columns = [f\"{contest['Slug']}_{col}\" for col in top_terms_df.columns]\n",
    "    all_results.append(top_terms_df)\n",
    "\n",
    "# Combine all topic term results into a single dataframe\n",
    "if all_results:\n",
    "    results = pd.concat(all_results, axis=1)\n",
    "    print(\"\\nLDA Analysis Complete. Top terms extracted:\")\n",
    "    display(results.head())\n",
    "else:\n",
    "    print(\"LDA Analysis did not produce any results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at the word clusters for each topic (e.g., `RF4_Topic_1`), we can assign a human-readable label. For instance, a topic with words like \"communiti,\" \"blockchain,\" \"develop,\" \"secur,\" and \"support\" is possibly about **Governance**.\n",
    "\n",
    "<div style=\"display: flex; justify-content: space-between; align-items: flex-end;\">\n",
    "  <img src=\"Shot_Poses_IACS/OmniacPoses_06.png\" width=\"110\"/>\n",
    "  <img src=\"Shot_Poses_IACS/OmniacPoses_07.png\" width=\"110\"/>\n",
    "  <img src=\"Shot_Poses_IACS/OmniacPoses_08.png\" width=\"110\"/>\n",
    "</div>\n",
    "\n",
    "## 5. Synthesizing Insights with the OpenAI API\n",
    "\n",
    "We now have structured data: n-grams and topics. But we still need to produce a high-level summary. This is where an LLM like GPT-4o mini shines. We will ask the AI to act as an analyst, providing it with our topic keywords and the raw submissions, and instructing it to generate a synthesized report.\n",
    "\n",
    "**Step 1**: Prepare the Prompt\n",
    "\n",
    "The key to getting a good result from an LLM is a well-structured prompt. Our prompt will:\n",
    "\n",
    "1. **Assign a role**: \"You are an expert blockchain community analyst.\"\n",
    "2. **Provide context**: Give it the top keywords from our LDA model.\n",
    "3. **Provide the data**: Give it the raw text submissions.\n",
    "4. **Give clear instructions**: Ask it to identify themes and find standout submissions.\n",
    "5. **Specify the output format**: Request clear headings for easy reading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- OpenAI Summarization ---\n",
    "\n",
    "# Let's focus on the \"Reduce Friction\" contest (RF4)\n",
    "slug_to_summarize = 'RF4'\n",
    "all_submissions = []\n",
    "\n",
    "# Extract top words for the chosen contest\n",
    "try:\n",
    "    rf4_cols = [col for col in results.columns if col.startswith(f\"{slug_to_summarize}_Topic_\")]\n",
    "    top_words_series = results[rf4_cols].stack().unique()\n",
    "    top_words = ', '.join(top_words_series)\n",
    "\n",
    "    # Load all submission text files for the chosen contest\n",
    "    contest_folder = os.path.join(output_dir, slug_to_summarize)\n",
    "    for filename in os.listdir(contest_folder):\n",
    "        if filename.endswith('.txt'):\n",
    "            with open(os.path.join(contest_folder, filename), 'r', encoding='utf-8') as f:\n",
    "                all_submissions.append(f.read())\n",
    "    submissions_text = '\\n'.join(all_submissions)\n",
    "    submissions_text = submissions_text[:8000] # Truncate to avoid exceeding token limits\n",
    "\n",
    "    # Craft the prompt\n",
    "    prompt = f\"\"\"\n",
    "    You are an expert blockchain community analyst. Given the following user submissions and most common keywords, create a detailed summary for each of these categories:\n",
    "    1. Reduce Friction (overall)\n",
    "    2. Growth and Innovation\n",
    "    3. Vision\n",
    "    4. Mission\n",
    "\n",
    "    For each category, do the following:\n",
    "    - Identify and describe the main themes, concerns, and opportunities found in the submissions.\n",
    "    - Provide at least 2–3 representative or standout submissions (either as direct quotes or short paraphrases) that best capture the spirit or key insights of that category.\n",
    "    - Where relevant, highlight points of consensus, strong opinions, or recurring challenges and suggestions.\n",
    "\n",
    "    Most common keywords:\n",
    "    {top_words}\n",
    "\n",
    "    User submissions:\n",
    "    {submissions_text}\n",
    "\n",
    "    Format your output with clear headings for each category, and organize within each section as:\n",
    "    - Themes:\n",
    "      [Detailed synthesis]\n",
    "    - Standout submissions:\n",
    "      - [Quote or paraphrase 1]\n",
    "      - [Quote or paraphrase 2]\n",
    "      - [Quote or paraphrase 3]\n",
    "\n",
    "    If a submission is relevant to more than one category, you may include it in multiple sections.\n",
    "    \"\"\"\n",
    "except (NameError, FileNotFoundError) as e:\n",
    "    print(f\"Could not prepare prompt. Did the LDA analysis for '{slug_to_summarize}' run successfully?\")\n",
    "    prompt = \"\" # Set prompt to empty to prevent API call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2**: Make the API Call\n",
    "\n",
    "Next, we send our prompt to the OpenAI API using the `requests` package. For this exercise, you will need an API key from OpenAI. \n",
    "\n",
    "**Important**: Never hardcode your API key directly in your script. For ease of use, load it from a separate, secure file (that you don't share or commit to version control). For a more advanced secure approach, store your API key as a system environment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Call OpenAI API ---\n",
    "api_key = None\n",
    "try:\n",
    "    # Load your API key securely from a file (e.g., a file named 'openai_key.txt' in a parent directory)\n",
    "    with open('../../openai_key.txt', 'r') as f:\n",
    "        api_key = f.read().strip()\n",
    "except FileNotFoundError:\n",
    "    print(\"OpenAI API key file not found. Please create a file named 'openai_key.txt' in the parent directory.\")\n",
    "\n",
    "output_box_style = \"\"\"\n",
    "<style>\n",
    ".output-box {\n",
    "  border: 2px solid #18bc9c;\n",
    "  background: #eafaf1;\n",
    "  padding: 12px;\n",
    "  margin: 1em 0;\n",
    "  border-radius: 6px;\n",
    "  font-size: 1.0em;\n",
    "  font-family: \"Fira Mono\", \"Menlo\", \"Monaco\", \"Consolas\", monospace;\n",
    "  white-space: pre-wrap;\n",
    "}\n",
    "</style>\n",
    "\"\"\"\n",
    "\n",
    "if api_key and prompt:\n",
    "    headers = {\n",
    "        'Authorization': f'Bearer {api_key}',\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "    payload = {\n",
    "        'model': 'gpt-4o-mini',\n",
    "        'messages': [{'role': 'user', 'content': prompt}],\n",
    "        'temperature': 0.5,\n",
    "        'max_tokens': 1500\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post('https://api.openai.com/v1/chat/completions', headers=headers, json=payload)\n",
    "        response.raise_for_status()  # Raise an exception for bad status codes (4xx or 5xx)\n",
    "        \n",
    "        result = response.json()\n",
    "        content = result['choices'][0]['message']['content']\n",
    "        display(HTML(f\"{output_box_style}<div class='output-box'>{content}</div>\"))\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        error_content = e.response.text if e.response else str(e)\n",
    "        display(HTML(f\"{output_box_style}<div class='output-box'>Error calling OpenAI API:<br>{error_content}</div>\"))\n",
    "else:\n",
    "    display(HTML(f\"{output_box_style}<div class='output-box'>Skipping OpenAI API call. API key or prompt not available.</div>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right;\">\n",
    "  <img src=\"Shot_Poses_IACS/OmniacPoses_09.png\" width=\"150\"/>\n",
    "</div>\n",
    "\n",
    "## 6. Conclusion and Recommendations\n",
    "\n",
    "By combining traditional NLP techniques with the power of modern LLMs, we have moved from thousands of raw text submissions to a concise, actionable summary of community feedback.\n",
    "\n",
    "**Key Findings**\n",
    "\n",
    "- **Identified Core Themes**: Using LDA, we successfully identified and categorized the main topics of conversation, such as governance, onboarding, and developer tooling.\n",
    "- **Pinpointed Specific Suggestions**: N-gram analysis helped highlight specific, recurring phrases and proposals.\n",
    "- **Synthesized Actionable Insights**: The OpenAI API provided a high-quality narrative summary, saving hours of manual reading and interpretation.\n",
    "\n",
    "**Recommendations for Future Analysis**\n",
    "\n",
    "- **Improve Prompting**: To get more structured data from the community, design submission forms with clearer, more focused prompts.\n",
    "- **Automate Filtering**: Use regular expressions and readability metrics to automatically filter out low-quality or spam submissions.\n",
    "- **Integrate Voting Data**: Correlate the topics of proposals with their voting outcomes to see which ideas gained the most traction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
